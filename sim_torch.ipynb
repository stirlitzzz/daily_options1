{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from copy import deepcopy\n",
    "import py_vollib_vectorized\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)  # Output Q-values for each action\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Target network starts as a copy\n",
    "        self.target_model.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0  # Start with full exploration\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.memory = []  # Replay buffer\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def select_action(self, state, valid_actions):\n",
    "        \"\"\" Selects an action using epsilon-greedy and masks invalid actions \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)  # Random valid action\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        q_values = self.model(state_tensor).cpu().detach().numpy().flatten()\n",
    "\n",
    "        # Mask invalid actions\n",
    "        masked_q_values = np.full_like(q_values, -np.inf)\n",
    "        masked_q_values[valid_actions] = q_values[valid_actions]\n",
    "\n",
    "        return np.argmax(masked_q_values)\n",
    "\n",
    "    def store_transition(self, transition):\n",
    "        \"\"\" Stores a transition (s, a, r, s', done) in the replay buffer \"\"\"\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > 10_000:  # Keep buffer size manageable\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" Trains the DQN with a batch from replay buffer \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Compute Q-values\n",
    "        q_values = self.model(states)\n",
    "        #print(f\"q_values: {q_values}\")\n",
    "        #print(f\"actions: {actions}\")\n",
    "        #print(f\"q_values.shape: {q_values.shape}\")\n",
    "        #print(\"actions.shape: \", actions.shape)\n",
    "        #print(\"actions.unsqueeze(1).shape: \", actions.unsqueeze(1).shape)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Select Q-values for taken actions\n",
    "\n",
    "        # Compute target Q-values using target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_model(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\" Updates target network weights with main model weights \"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quadratic_volatility_model(strikes, spot, atm_vol, slope, quadratic_term, texp_years):\n",
    "    \"\"\"\n",
    "    Apply the quadratic volatility model to new data points.\n",
    "    \n",
    "    Parameters:\n",
    "        strikes (array-like): Array of strike prices.\n",
    "        spot (float): Spot price.\n",
    "        atm_vol (float): At-the-money volatility.\n",
    "        slope (float): Slope of the linear term.\n",
    "        quadratic_term (float): Coefficient of the quadratic term.\n",
    "        texp_years (float): Time to expiration in years.\n",
    "    \n",
    "    Returns:\n",
    "        array-like: Fitted volatilities for the given strikes.\n",
    "    \"\"\"\n",
    "    #print(f\"apply_quadratic_vol input sizes: strikes={strikes}, spot={len(spot)}, atm_vol={len(atm_vol)}, slope={len(slope)}, quadratic_term={len(quadratic_term)}, texp_years={len(texp_years)}\")\n",
    "    log_strikes = np.log(strikes) - np.log(spot)\n",
    "    fitted_vols = atm_vol + slope * log_strikes + quadratic_term * log_strikes**2\n",
    "    #fitted_vols = atm_vol + (slope / np.sqrt(texp_years)) * log_strikes + quadratic_term * log_strikes**2\n",
    "    fitted_vols= np.clip(fitted_vols, .05,.4)\n",
    "    return fitted_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/dv7w7b2x49j3spqsxnz6y_sw0000gn/T/ipykernel_74813/1781164678.py:2: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df['minute'] = pd.to_datetime(df['minute'])\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"./algo_data/vol_surfaces2.csv\")\n",
    "df['minute'] = pd.to_datetime(df['minute'])\n",
    "df['minute'].apply(lambda x: x.tz).unique()\n",
    "#for each row find 16:17:00 and compute years to maturity where maturity is 16:17:00 for each row\n",
    "\n",
    "def get_years_to_maturity(row):\n",
    "    maturity = pd.Timestamp(row['minute'].date(), tz=row['minute'].tz) + pd.Timedelta(hours=16, minutes=17)\n",
    "    return (maturity - row['minute']).seconds / (365.25 * 24 * 60 * 60)\n",
    "\n",
    "df['years_to_maturity'] = df.apply(get_years_to_maturity, axis=1)\n",
    "df.loc[df['implied_spot'] <= .07, ['implied_spot', 'atm_vol', 'slope', 'quadratic_term', 'scaled_slope', 'scaled_quadratic']] = np.nan\n",
    "df.loc[df['atm_vol'] <= .03, ['implied_spot', 'atm_vol', 'slope', 'quadratic_term', 'scaled_slope', 'scaled_quadratic']] = np.nan\n",
    "# Forward fill the NaN values\n",
    "df=df.ffill().infer_objects(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym  # ✅ Use gymnasium instead of gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "\n",
    "class SimEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Options Trading Environment for Reinforcement Learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        super(SimEnv, self).__init__()\n",
    "        \n",
    "        # Market Data\n",
    "        self.df = df\n",
    "        self.df_today = None\n",
    "        \n",
    "        # Index Tracking\n",
    "        self.global_index = 0\n",
    "        self.daily_index = 0\n",
    "        self.start_index = 0\n",
    "        self.max_steps = 62  # Max steps per episode\n",
    "\n",
    "        # Trading Variables\n",
    "        self.position = 0\n",
    "        self.entry_price = 0\n",
    "        self.position_open_time = None\n",
    "\n",
    "        # Capital & PnL\n",
    "        self.capital = 100\n",
    "        self.pnl = 0\n",
    "        self.position_value = 0\n",
    "\n",
    "        # Episode State\n",
    "        self.done = False\n",
    "        self.current_row = None\n",
    "        self.last_pnl = 0\n",
    "\n",
    "        # Action & Observation Space\n",
    "        self.action_space = spaces.Discrete(3)  # 0: Hold, 1: Open, 2: Close\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(11,), dtype=np.float32)\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        super(SimEnv, self).__init__()\n",
    "        \n",
    "        # ✅ Read `df` from `env_config`\n",
    "        self.df = env_config.get(\"df\")\n",
    "        \n",
    "        # ✅ Ensure df is provided\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Error: `df` must be provided in env_config!\")\n",
    "        \n",
    "        # ✅ Initialize other attributes\n",
    "        self.df_today = None\n",
    "        self.global_index = 0\n",
    "        self.max_steps = 62  # Max steps per episode\n",
    "\n",
    "        self.position = 0\n",
    "        self.entry_price = 0\n",
    "        self.pnl = 0\n",
    "        self.done = False\n",
    "        self.current_row = None\n",
    "        self.last_pnl = 0\n",
    "        self.capital = 100\n",
    "        \n",
    "        self.action_space = spaces.Discrete(3)  # 0: Hold, 1: Open, 2: Close\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(11,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        \"\"\" Reset environment and start a new episode. \"\"\"\n",
    "        self.global_index = self.pick_episode_start()\n",
    "        self.daily_index = self.df_today.index.get_loc(self.global_index)\n",
    "        self.start_index = self.global_index\n",
    "        self.done = False\n",
    "        self.position = 0\n",
    "        self.pnl = 0\n",
    "        self.capital = 100\n",
    "        # Compute daily straddle prices before trading starts\n",
    "        straddle_prices = self.compute_daily_atm_straddle_prices()\n",
    "        self.df_today[\"daily_straddle_prices\"] = straddle_prices\n",
    "        self.df_today[\"open_straddle_pnl\"] = 0\n",
    "\n",
    "        obs = self._get_state()  # Observation (state)\n",
    "        action_mask = self.compute_action_mask()  # ✅ Compute action mask for valid actions\n",
    "        info = {\"action_mask\": action_mask}  # ✅ Include action mask in `info`\n",
    "\n",
    "        return obs, info  # ✅ Must return a tuple (obs, info)\n",
    "\n",
    "    def compute_action_mask(self):\n",
    "        \"\"\" Computes an action mask where invalid actions are marked as 0. \"\"\"\n",
    "        action_mask = np.array([1, 1, 1])  # Default: all actions allowed\n",
    "        \n",
    "        if self.position == 0:\n",
    "            action_mask[2] = 0  # Can't close if no position is open\n",
    "        else:\n",
    "            action_mask[1] = 0  # Can't open a new position if one is already open\n",
    "        \n",
    "        return action_mask  # ✅ Masked actions for Rllib\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" Execute the selected action. \"\"\"\n",
    "        reward = 0.0\n",
    "        allowed_actions = self.valid_actions()\n",
    "\n",
    "        if action not in allowed_actions:\n",
    "            reward = -1000  # Strong penalty for invalid action\n",
    "            truncated = True\n",
    "            self.done = True\n",
    "            return self._get_state(), reward, self.done, truncated, {\"action_mask\": self.compute_action_mask()}  # ✅ Return action mask\n",
    "\n",
    "        if action == 0 and self.position == 0:  # Open position\n",
    "            self.open_position()\n",
    "            self.update_time_step(60)\n",
    "\n",
    "        elif action == 1 and self.position > 0:  # Close position\n",
    "            reward = self.df_today[\"open_straddle_pnl\"].loc[self.global_index] - self.pnl\n",
    "            self.pnl = self.df_today[\"open_straddle_pnl\"].loc[self.global_index]\n",
    "            self.position = 0\n",
    "            self.done = True\n",
    "\n",
    "        elif action == 2:  # Hold position\n",
    "            if self.position > 0:\n",
    "                reward = self.df_today[\"open_straddle_pnl\"].loc[self.global_index] - self.pnl\n",
    "                self.pnl = self.df_today[\"open_straddle_pnl\"].loc[self.global_index]\n",
    "            \n",
    "            self.update_time_step(1)\n",
    "\n",
    "        # End episode if time exceeds max steps\n",
    "        if self.global_index - self.start_index >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_state(), reward, self.done, False, {\"action_mask\": self.compute_action_mask()}  # ✅ Return action mask\n",
    "    \n",
    "    def pick_random_day(self, burn_days=5):\n",
    "        all_days = self.df['date'].unique()\n",
    "        all_days = sorted(all_days)\n",
    "        start_day = np.random.choice(all_days[burn_days:-1])\n",
    "        return start_day\n",
    "\n",
    "    def pick_random_timestep(self,df):\n",
    "        all_times = self.df['minute'].apply(lambda x: x.time()).unique()\n",
    "        all_times = sorted(all_times)\n",
    "        latest_time = pd.Timestamp('12:45').time()\n",
    "        earliest_time = pd.Timestamp('9:30').time()\n",
    "        all_times = [x for x in all_times if x >= earliest_time and x <= latest_time]\n",
    "        start_time = np.random.choice(all_times)\n",
    "        return start_time\n",
    "\n",
    "    def pick_episode_start(self):\n",
    "        start_day = self.pick_random_day()\n",
    "        self.df_today = self.df[self.df['date'] == start_day]\n",
    "        self.df_today=deepcopy(self.df_today)\n",
    "        start_time=self.pick_random_timestep(self.df_today)\n",
    "        episode_start_index = self.df[(self.df['date'] == start_day) & (self.df['minute'].apply(lambda x: x.time()) == start_time)].index[0]\n",
    "        \n",
    "        #self.current_row = self.df.iloc[self.global_index]\n",
    "        #self.df_today=self.select_todays_data()\n",
    "        return episode_start_index\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def compute_spot_vols(self,strike):\n",
    "        \"\"\"\n",
    "        Compute fitted volatilities for a range of strikes.\n",
    "        \n",
    "        Parameters:\n",
    "            spot (float): Spot price.\n",
    "            atm_vol (float): At-the-money\n",
    "            slope (float): Slope of the linear term.\n",
    "            quadratic_term (float): Coefficient of the quadratic term.\n",
    "            texp_years (float): Time to expiration in years.    \n",
    "\n",
    "        Returns:\n",
    "            array-like: Fitted volatilities for a range of strikes.\n",
    "        \"\"\"\n",
    "        spots=self.df_today['implied_spot']\n",
    "        atm_vol=self.df_today['atm_vol']\n",
    "        texp_years = self.df_today['years_to_maturity']\n",
    "        slope=self.df_today['slope']\n",
    "        quadratic_term=self.df_today['quadratic_term']\n",
    "        #print(f\"variable sizes: texp={texp_years.shape}, spot={spots.shape}, atm_vol={atm_vol.shape}, slope={slope.shape}, quadratic_term={quadratic_term.shape},strike={strike.shape}\")\n",
    "        vols = apply_quadratic_volatility_model(strike, spots, atm_vol, slope, quadratic_term, texp_years)\n",
    "        #print(f\"vols size={vols.shape}\")\n",
    "        return vols\n",
    "\n",
    "\n",
    "    def compute_daily_atm_straddle_prices(self):\n",
    "        \"\"\"\n",
    "        Compute straddle prices for a range of strikes.\n",
    "        \n",
    "        Parameters:\n",
    "            spot (float): Spot price.\n",
    "            atm_vol (float): At-the-money\n",
    "            slope (float): Slope of the linear term.\n",
    "            quadratic_term (float): Coefficient of the quadratic term.\n",
    "            texp_years (float): Time to expiration in years.    \n",
    "\n",
    "        Returns:\n",
    "            array-like: Fitted volatilities for a range of strikes.\n",
    "        \"\"\"\n",
    "        texp = self.df_today['years_to_maturity']\n",
    "        spot = self.df_today['implied_spot']\n",
    "        texp = self.df_today['years_to_maturity']\n",
    "        vol=self.df_today['atm_vol']\n",
    "        #print(\"variable sizes: \",texp.shape,spot.shape,vol.shape)\n",
    "        straddle_prices = self.price_instrument('c', spot, spot, texp, vol) + self.price_instrument('p', spot, spot, texp, vol)\n",
    "\n",
    "        return straddle_prices\n",
    "\n",
    "    \n",
    "    def compute_straddle_prices(self, strike):\n",
    "        \"\"\"\n",
    "        Compute straddle prices for a range of strikes.\n",
    "        \n",
    "        Parameters:\n",
    "            spot (float): Spot price.\n",
    "            atm_vol (float): At-the-money\n",
    "            slope (float): Slope of the linear term.\n",
    "            quadratic_term (float): Coefficient of the quadratic term.\n",
    "            texp_years (float): Time to expiration in years.    \n",
    "\n",
    "        Returns:\n",
    "            array-like: Fitted volatilities for a range of strikes.\n",
    "        \"\"\"\n",
    "    \n",
    "        texp = self.df_today['years_to_maturity']\n",
    "        spot = self.df_today['implied_spot']\n",
    "        vols=self.compute_spot_vols(strike)\n",
    "        #print(f\"variable sizes: texp={texp.shape}, spot={spot.shape}, vols={vols.shape}\")\n",
    "        #vols=apply_apply_quadratic_volatility_model(strike, spot, atm_vols, slopes, quadratic_terms, texp)\n",
    "        straddle_prices = self.price_instrument('c', strike, spot, texp, vols) + self.price_instrument('p', strike, spot, texp, vols) \n",
    "        #print(f\"straddle_prices={straddle_prices}\")\n",
    "\n",
    "        df_output=pd.DataFrame()\n",
    "    \n",
    "        df_output[\"spot\"]=spot\n",
    "        df_output[\"texp\"]=texp\n",
    "        df_output[\"vols\"]=vols\n",
    "        df_output[\"strike\"]=strike\n",
    "        df_output[\"straddle_prices\"]=straddle_prices\n",
    "        df_output.to_csv(\"straddle_prices.csv\")\n",
    "\n",
    "        return straddle_prices\n",
    "\n",
    "\n",
    "    def update_time_step(self, minutes=1):\n",
    "        self.global_index = min(self.global_index + minutes, self.df_today.index.max())\n",
    "\n",
    "    def price_instrument(self, cp, strike, spot, texp, vol):\n",
    "        #if self.debug:\n",
    "        #    print(f\"cp={cp}\\n, strike={strike}\\n, spot={spot}\\n, texp={texp}\\n, vol={vol}\\n\")\n",
    "        #print(f\"pricing_insturment sizes: cp={cp}, strike={strike.shape}, spot={spot.shape}, texp={texp.shape}, vol={vol.shape}\")\n",
    "        return py_vollib_vectorized.models.vectorized_black_scholes(cp, spot, strike, texp, 0, vol,return_as=\"numpy\")\n",
    "\n",
    "    def update_time_step(self, minutes=1):\n",
    "        self.global_index = min(self.global_index + minutes, len(self.df) - 1)\n",
    "        self.daily_index = min(self.daily_index + minutes, len(self.df_today) - 1) \n",
    "\n",
    "    def get_current_time(self):\n",
    "        return self.df.iloc[self.global_index]['minute']\n",
    "    \n",
    "\n",
    "    def get_current_row(self):\n",
    "        return self.df.iloc[self.global_index]\n",
    "\n",
    "    def open_position(self):\n",
    "\n",
    "        ivol = self.get_current_row()['implied_spot']\n",
    "        texp = self.get_current_row()['years_to_maturity']\n",
    "        spot=self.get_current_row()['implied_spot']\n",
    "        #straddle_price_1 = self.price_one_day_straddle(texp, ivol)\n",
    "        straddle_price=self.df_today['daily_straddle_prices'].loc[self.global_index]\n",
    "        #print(f\"straddle_price={straddle_price}\")\n",
    "        #print(f\"straddle_price_1={straddle_price_1}\")\n",
    "        if (straddle_price == 0):\n",
    "            print(f\"eror: straddle_price={straddle_price}. at time={self.get_current_time()}\")\n",
    "        self.position = self.capital / straddle_price\n",
    "        #self.position_value = self.position * straddle_price\n",
    "        self.strike=spot\n",
    "        #spot_vols=self.compute_spot_vols(self.strike)\n",
    "        self.straddle_prices=self.compute_straddle_prices(self.strike)\n",
    "        self.df_today[\"open_straddle_prices\"]=self.straddle_prices\n",
    "        self.df_today[\"open_straddle_pnl\"]=(self.df_today[\"open_straddle_prices\"]- straddle_price)*self.position\n",
    "        self.position_open_time = self.global_index\n",
    "        return self.position*straddle_price\n",
    "\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\" Returns the current state as a NumPy array. \"\"\"\n",
    "        row = self.df.iloc[self.global_index]\n",
    "        steps_taken = self.global_index - self.start_index\n",
    "        steps_remaining = self.max_steps - steps_taken\n",
    "        \n",
    "        state = np.array([\n",
    "            row['implied_spot'],  # Current spot price\n",
    "            row['atm_vol'],  # ATM implied volatility\n",
    "            row['scaled_slope'],  # Volatility skew slope\n",
    "            row['scaled_quadratic'],  # Volatility skew curvature\n",
    "            steps_taken,  # How many steps taken in this episode\n",
    "            steps_remaining,  # Steps remaining before timeout\n",
    "            self.position,  # Position status (0: no position, >0: position held)\n",
    "            int(self.position>0),  # binary state to make it sumpler\n",
    "            self.pnl,  # Cumulative PnL\n",
    "            self.df_today[\"daily_straddle_prices\"].loc[self.global_index],  # Current straddle price\n",
    "            self.df_today[\"open_straddle_pnl\"].loc[self.global_index]  # PnL from position\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def valid_actions(self):\n",
    "        if self.position == 0:\n",
    "            return [0, 2]\n",
    "        else:\n",
    "            return [1, 2]\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\" Optional: Print state information for debugging. \"\"\"\n",
    "        print(f\"Time: {self.df.iloc[self.global_index]['minute']}, Position: {self.position}, PnL: {self.pnl}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -9.191996805577897, Epsilon: 1.00\n",
      "Episode 1, Reward: -0.3128137749859505, Epsilon: 1.00\n",
      "Episode 2, Reward: -8.191066900460113, Epsilon: 1.00\n",
      "Episode 3, Reward: -14.376436552661081, Epsilon: 1.00\n",
      "Episode 4, Reward: -4.680182134762129, Epsilon: 1.00\n",
      "Episode 5, Reward: -8.328714255619682, Epsilon: 1.00\n",
      "Episode 6, Reward: 15.13748828396314, Epsilon: 1.00\n",
      "Episode 7, Reward: -10.275232323484124, Epsilon: 1.00\n",
      "Episode 8, Reward: 0.0, Epsilon: 1.00\n",
      "Episode 9, Reward: -8.469761039302318, Epsilon: 1.00\n",
      "Episode 10, Reward: -11.24579658689321, Epsilon: 1.00\n",
      "Episode 11, Reward: 0.0, Epsilon: 0.96\n",
      "Episode 12, Reward: -15.183636931852321, Epsilon: 0.95\n",
      "Episode 13, Reward: 0.0, Epsilon: 0.92\n",
      "Episode 14, Reward: -12.529637165691472, Epsilon: 0.91\n",
      "Episode 15, Reward: -11.519170177676004, Epsilon: 0.90\n",
      "Episode 16, Reward: -14.295216988519385, Epsilon: 0.89\n",
      "Episode 17, Reward: -19.133734551607755, Epsilon: 0.88\n",
      "Episode 18, Reward: 23.82630765261593, Epsilon: 0.87\n",
      "Episode 19, Reward: 0.0, Epsilon: 0.86\n",
      "Episode 20, Reward: 0.0, Epsilon: 0.84\n",
      "Episode 21, Reward: -9.03597553332077, Epsilon: 0.83\n",
      "Episode 22, Reward: -8.664984431694615, Epsilon: 0.81\n",
      "Episode 23, Reward: 0.0, Epsilon: 0.79\n",
      "Episode 24, Reward: -14.666029088997684, Epsilon: 0.78\n",
      "Episode 25, Reward: 25.29120561574313, Epsilon: 0.77\n",
      "Episode 26, Reward: 19.18139311594241, Epsilon: 0.76\n",
      "Episode 27, Reward: -1.887219378811352, Epsilon: 0.75\n",
      "Episode 28, Reward: -2.0876091256652622, Epsilon: 0.74\n",
      "Episode 29, Reward: -9.617337058985061, Epsilon: 0.73\n",
      "Episode 30, Reward: 6.854039200750154, Epsilon: 0.72\n",
      "Episode 31, Reward: 0.0, Epsilon: 0.71\n",
      "Episode 32, Reward: 0.0, Epsilon: 0.70\n",
      "Episode 33, Reward: 0.0, Epsilon: 0.69\n",
      "Episode 34, Reward: 15.088244022879675, Epsilon: 0.68\n",
      "Episode 35, Reward: -17.743127064107288, Epsilon: 0.67\n",
      "Episode 36, Reward: 0.0, Epsilon: 0.66\n",
      "Episode 37, Reward: -12.451754437801572, Epsilon: 0.65\n",
      "Episode 38, Reward: -12.946095104317795, Epsilon: 0.64\n",
      "Episode 39, Reward: 11.041314822330655, Epsilon: 0.63\n",
      "Episode 40, Reward: -15.82511713045595, Epsilon: 0.62\n",
      "Episode 41, Reward: -9.94136567768114, Epsilon: 0.61\n",
      "Episode 42, Reward: -12.600634968453166, Epsilon: 0.60\n",
      "Episode 43, Reward: 3.5269275105669684, Epsilon: 0.60\n",
      "Episode 44, Reward: -25.223711607514147, Epsilon: 0.59\n",
      "Episode 45, Reward: 8.04986426905959, Epsilon: 0.58\n",
      "Episode 46, Reward: 10.379464713433983, Epsilon: 0.57\n",
      "Episode 47, Reward: -14.93012203525952, Epsilon: 0.56\n",
      "Episode 48, Reward: 0.0, Epsilon: 0.55\n",
      "Episode 49, Reward: 0.0, Epsilon: 0.54\n",
      "Episode 50, Reward: 0.0, Epsilon: 0.53\n",
      "Episode 51, Reward: 0.0, Epsilon: 0.52\n",
      "Episode 52, Reward: -0.5516561251329422, Epsilon: 0.51\n",
      "Episode 53, Reward: -12.045874643849666, Epsilon: 0.50\n",
      "Episode 54, Reward: -10.161745003585441, Epsilon: 0.50\n",
      "Episode 55, Reward: -19.05294006908834, Epsilon: 0.49\n",
      "Episode 56, Reward: -12.405016395491474, Epsilon: 0.48\n",
      "Episode 57, Reward: -13.788204184581724, Epsilon: 0.48\n",
      "Episode 58, Reward: 0.0, Epsilon: 0.46\n",
      "Episode 59, Reward: -15.461946519578206, Epsilon: 0.46\n",
      "Episode 60, Reward: -17.686370855674408, Epsilon: 0.45\n",
      "Episode 61, Reward: -11.528658196623521, Epsilon: 0.44\n",
      "Episode 62, Reward: -9.418026280271219, Epsilon: 0.44\n",
      "Episode 63, Reward: -2.0748508538429813, Epsilon: 0.43\n",
      "Episode 64, Reward: 0.0, Epsilon: 0.43\n",
      "Episode 65, Reward: 13.57948882708786, Epsilon: 0.42\n",
      "Episode 66, Reward: -3.2732140269428887, Epsilon: 0.42\n",
      "Episode 67, Reward: -9.241681502946122, Epsilon: 0.41\n",
      "Episode 68, Reward: 0.0, Epsilon: 0.40\n",
      "Episode 69, Reward: 0.0, Epsilon: 0.40\n",
      "Episode 70, Reward: -16.230117741248986, Epsilon: 0.39\n",
      "Episode 71, Reward: 0.0, Epsilon: 0.38\n",
      "Episode 72, Reward: -4.12246958395992, Epsilon: 0.38\n",
      "Episode 73, Reward: 0.0, Epsilon: 0.37\n",
      "Episode 74, Reward: 0.0, Epsilon: 0.36\n",
      "Episode 75, Reward: 0.0, Epsilon: 0.35\n",
      "Episode 76, Reward: 0.0, Epsilon: 0.34\n",
      "Episode 77, Reward: 0.0, Epsilon: 0.33\n",
      "Episode 78, Reward: -3.1619877957371525, Epsilon: 0.33\n",
      "Episode 79, Reward: 0.0, Epsilon: 0.32\n",
      "Episode 80, Reward: 9.492022574958023, Epsilon: 0.31\n",
      "Episode 81, Reward: 0.0, Epsilon: 0.31\n",
      "Episode 82, Reward: 10.280269519095647, Epsilon: 0.30\n",
      "Episode 83, Reward: 0.0, Epsilon: 0.30\n",
      "Episode 84, Reward: -2.561316225105197, Epsilon: 0.29\n",
      "Episode 85, Reward: 0.0, Epsilon: 0.29\n",
      "Episode 86, Reward: 0.4177974655095581, Epsilon: 0.29\n",
      "Episode 87, Reward: 0.0, Epsilon: 0.28\n",
      "Episode 88, Reward: 0.0, Epsilon: 0.27\n",
      "Episode 89, Reward: -8.790243802770433, Epsilon: 0.27\n",
      "Episode 90, Reward: 0.0, Epsilon: 0.26\n",
      "Episode 91, Reward: 0.0, Epsilon: 0.25\n",
      "Episode 92, Reward: -4.1849071244667435, Epsilon: 0.25\n",
      "Episode 93, Reward: 0.0, Epsilon: 0.24\n",
      "Episode 94, Reward: 0.0, Epsilon: 0.24\n",
      "Episode 95, Reward: -8.576003592746975, Epsilon: 0.24\n",
      "Episode 96, Reward: 0.0, Epsilon: 0.22\n",
      "Episode 97, Reward: -16.050301033352753, Epsilon: 0.22\n",
      "Episode 98, Reward: -8.940534567949696, Epsilon: 0.22\n",
      "Episode 99, Reward: 0.0, Epsilon: 0.21\n",
      "Episode 100, Reward: 0.0, Epsilon: 0.20\n",
      "Episode 101, Reward: 36.3456457997776, Epsilon: 0.20\n",
      "Episode 102, Reward: 23.206162171132995, Epsilon: 0.20\n",
      "Episode 103, Reward: -3.382908490328207, Epsilon: 0.20\n",
      "Episode 104, Reward: -13.243766321420665, Epsilon: 0.19\n",
      "Episode 105, Reward: -8.791798040809569, Epsilon: 0.19\n",
      "Episode 106, Reward: -6.221813583235328, Epsilon: 0.19\n",
      "Episode 107, Reward: -6.3891133711285715, Epsilon: 0.19\n",
      "Episode 108, Reward: 0.0, Epsilon: 0.18\n",
      "Episode 109, Reward: -5.1733273470026955, Epsilon: 0.18\n",
      "Episode 110, Reward: 24.28715241128803, Epsilon: 0.18\n",
      "Episode 111, Reward: 3.4772706426183833, Epsilon: 0.18\n",
      "Episode 112, Reward: -23.07147688284538, Epsilon: 0.17\n",
      "Episode 113, Reward: 36.4778115090512, Epsilon: 0.17\n",
      "Episode 114, Reward: 12.702993983989357, Epsilon: 0.17\n",
      "Episode 115, Reward: 0.0, Epsilon: 0.17\n",
      "Episode 116, Reward: 35.99890857374877, Epsilon: 0.16\n",
      "Episode 117, Reward: -15.842850212808491, Epsilon: 0.16\n",
      "Episode 118, Reward: 16.433692402243636, Epsilon: 0.16\n",
      "Episode 119, Reward: 0.0, Epsilon: 0.16\n",
      "Episode 120, Reward: -2.8567097987933874, Epsilon: 0.15\n",
      "Episode 121, Reward: -0.27557794872875213, Epsilon: 0.15\n",
      "Episode 122, Reward: 0.0, Epsilon: 0.15\n",
      "Episode 123, Reward: -14.524115900976183, Epsilon: 0.15\n",
      "Episode 124, Reward: 10.217627759914368, Epsilon: 0.14\n",
      "Episode 125, Reward: -4.034315831215985, Epsilon: 0.14\n",
      "Episode 126, Reward: 7.990490112068021, Epsilon: 0.14\n",
      "Episode 127, Reward: 0.0, Epsilon: 0.14\n",
      "Episode 128, Reward: 0.0, Epsilon: 0.14\n",
      "Episode 129, Reward: -5.858418800302493, Epsilon: 0.13\n",
      "Episode 130, Reward: 0.668069545857632, Epsilon: 0.13\n",
      "Episode 131, Reward: -5.118538089982021, Epsilon: 0.13\n",
      "Episode 132, Reward: -12.063283587774007, Epsilon: 0.13\n",
      "Episode 133, Reward: 0.0, Epsilon: 0.13\n",
      "Episode 134, Reward: -0.428922045892284, Epsilon: 0.12\n",
      "Episode 135, Reward: -11.620058776011545, Epsilon: 0.12\n",
      "Episode 136, Reward: 8.631172017016338, Epsilon: 0.12\n",
      "Episode 137, Reward: -11.486546441606606, Epsilon: 0.12\n",
      "Episode 138, Reward: -18.1741465202968, Epsilon: 0.12\n",
      "Episode 139, Reward: -2.4000221975064364, Epsilon: 0.12\n",
      "Episode 140, Reward: 20.36341157048971, Epsilon: 0.12\n",
      "Episode 141, Reward: -9.020346313407329, Epsilon: 0.11\n",
      "Episode 142, Reward: -9.182110874384062, Epsilon: 0.11\n",
      "Episode 143, Reward: -11.835973322910883, Epsilon: 0.11\n",
      "Episode 144, Reward: -22.057338409442288, Epsilon: 0.11\n",
      "Episode 145, Reward: 72.15960973727638, Epsilon: 0.11\n",
      "Episode 146, Reward: -11.964404113011893, Epsilon: 0.11\n",
      "Episode 147, Reward: 14.656361947273927, Epsilon: 0.11\n",
      "Episode 148, Reward: -8.218728520230382, Epsilon: 0.10\n",
      "Episode 149, Reward: -3.627835376078587, Epsilon: 0.10\n",
      "Episode 150, Reward: -3.0451345634987073, Epsilon: 0.10\n",
      "Episode 151, Reward: 19.924189451320473, Epsilon: 0.10\n",
      "Episode 152, Reward: -17.705349600189585, Epsilon: 0.10\n",
      "Episode 153, Reward: -14.969529270386966, Epsilon: 0.10\n",
      "Episode 154, Reward: -3.6779995394067555, Epsilon: 0.10\n",
      "Episode 155, Reward: -7.168514396527229, Epsilon: 0.09\n",
      "Episode 156, Reward: 0.8884917534428474, Epsilon: 0.09\n",
      "Episode 157, Reward: -13.84167371641354, Epsilon: 0.09\n",
      "Episode 158, Reward: 56.05767692535416, Epsilon: 0.09\n",
      "Episode 159, Reward: 0.0, Epsilon: 0.09\n",
      "Episode 160, Reward: -10.304294379794301, Epsilon: 0.09\n",
      "Episode 161, Reward: -8.979115600560204, Epsilon: 0.08\n",
      "Episode 162, Reward: -10.971167169424596, Epsilon: 0.08\n",
      "Episode 163, Reward: -17.949598992293456, Epsilon: 0.08\n",
      "Episode 164, Reward: 6.124208436409305, Epsilon: 0.08\n",
      "Episode 165, Reward: 0.0, Epsilon: 0.08\n",
      "Episode 166, Reward: 2.4003765109771216, Epsilon: 0.08\n",
      "Episode 167, Reward: -21.676314288722455, Epsilon: 0.08\n",
      "Episode 168, Reward: -15.187364420455634, Epsilon: 0.07\n",
      "Episode 169, Reward: 2.8432152374915933, Epsilon: 0.07\n",
      "Episode 170, Reward: 65.43854703647024, Epsilon: 0.07\n",
      "Episode 171, Reward: 0.0, Epsilon: 0.07\n",
      "Episode 172, Reward: -13.919710545006163, Epsilon: 0.07\n",
      "Episode 173, Reward: -12.307462630615914, Epsilon: 0.07\n",
      "Episode 174, Reward: -9.410805058426082, Epsilon: 0.07\n",
      "Episode 175, Reward: -5.715934282317783, Epsilon: 0.07\n",
      "Episode 176, Reward: 0.0, Epsilon: 0.06\n",
      "Episode 177, Reward: -15.222243680561059, Epsilon: 0.06\n",
      "Episode 178, Reward: 0.0, Epsilon: 0.06\n",
      "Episode 179, Reward: -20.96618668922265, Epsilon: 0.06\n",
      "Episode 180, Reward: -12.906401570305537, Epsilon: 0.06\n",
      "Episode 181, Reward: 0.0, Epsilon: 0.06\n",
      "Episode 182, Reward: 80.21044698550047, Epsilon: 0.06\n",
      "Episode 183, Reward: -1.4691401972868128, Epsilon: 0.06\n",
      "Episode 184, Reward: 5.686663555018703, Epsilon: 0.06\n",
      "Episode 185, Reward: 0.0, Epsilon: 0.05\n",
      "Episode 186, Reward: 20.890856272787506, Epsilon: 0.05\n",
      "Episode 187, Reward: -1.097054142718662, Epsilon: 0.05\n",
      "Episode 188, Reward: -2.7924155039476797, Epsilon: 0.05\n",
      "Episode 189, Reward: -6.821241646102803, Epsilon: 0.05\n",
      "Episode 190, Reward: 0.0, Epsilon: 0.05\n",
      "Episode 191, Reward: 6.513844646189191, Epsilon: 0.05\n",
      "Episode 192, Reward: 3.730208185017405, Epsilon: 0.05\n",
      "Episode 193, Reward: 2.261386752808115, Epsilon: 0.05\n",
      "Episode 194, Reward: 0.0, Epsilon: 0.05\n",
      "Episode 195, Reward: -11.485645868785806, Epsilon: 0.05\n",
      "Episode 196, Reward: -8.439441852408013, Epsilon: 0.05\n",
      "Episode 197, Reward: 5.900388563583848, Epsilon: 0.05\n",
      "Episode 198, Reward: 10.16723270146104, Epsilon: 0.05\n",
      "Episode 199, Reward: 20.24672961275887, Epsilon: 0.04\n",
      "Episode 200, Reward: -9.629059578588262, Epsilon: 0.04\n",
      "Episode 201, Reward: 0.0, Epsilon: 0.04\n",
      "Episode 202, Reward: 23.936446923385358, Epsilon: 0.04\n",
      "Episode 203, Reward: -17.920517847171133, Epsilon: 0.04\n",
      "Episode 204, Reward: -14.383594440930635, Epsilon: 0.04\n",
      "Episode 205, Reward: -0.5288308895279088, Epsilon: 0.04\n",
      "Episode 206, Reward: 3.8104261257327234, Epsilon: 0.04\n",
      "Episode 207, Reward: 39.740065563835714, Epsilon: 0.04\n",
      "Episode 208, Reward: 0.0, Epsilon: 0.04\n",
      "Episode 209, Reward: 8.683592165646647, Epsilon: 0.04\n",
      "Episode 210, Reward: -11.384861090775658, Epsilon: 0.04\n",
      "Episode 211, Reward: -14.508511308303147, Epsilon: 0.04\n",
      "Episode 212, Reward: -13.641349413869891, Epsilon: 0.04\n",
      "Episode 213, Reward: 0.0, Epsilon: 0.03\n",
      "Episode 214, Reward: 0.0, Epsilon: 0.03\n",
      "Episode 215, Reward: 70.13933859265329, Epsilon: 0.03\n",
      "Episode 216, Reward: 0.0, Epsilon: 0.03\n",
      "Episode 217, Reward: -10.439043613159027, Epsilon: 0.03\n",
      "Episode 218, Reward: 0.0, Epsilon: 0.03\n",
      "Episode 219, Reward: 1.9582630760059179, Epsilon: 0.03\n",
      "Episode 220, Reward: -9.090363477089248, Epsilon: 0.03\n",
      "Episode 221, Reward: -10.252696789573433, Epsilon: 0.03\n",
      "Episode 222, Reward: 75.19010451972595, Epsilon: 0.03\n",
      "Episode 223, Reward: 0.0, Epsilon: 0.03\n",
      "Episode 224, Reward: -12.269461308921734, Epsilon: 0.03\n",
      "Episode 225, Reward: -4.668567014295439, Epsilon: 0.03\n",
      "Episode 226, Reward: -9.668666294936676, Epsilon: 0.03\n",
      "Episode 227, Reward: 0.0, Epsilon: 0.03\n",
      "Episode 228, Reward: -18.862610365439757, Epsilon: 0.03\n",
      "Episode 229, Reward: 1.7908943794477834, Epsilon: 0.03\n",
      "Episode 230, Reward: -7.637770516944711, Epsilon: 0.03\n",
      "Episode 231, Reward: -12.169994067732015, Epsilon: 0.02\n",
      "Episode 232, Reward: -4.397159285820433, Epsilon: 0.02\n",
      "Episode 233, Reward: -1.471760330513375, Epsilon: 0.02\n",
      "Episode 234, Reward: -8.353877955329118, Epsilon: 0.02\n",
      "Episode 235, Reward: -10.321892295556152, Epsilon: 0.02\n",
      "Episode 236, Reward: -2.3576386756367937, Epsilon: 0.02\n",
      "Episode 237, Reward: 2.261386752808115, Epsilon: 0.02\n",
      "Episode 238, Reward: -8.113726525116645, Epsilon: 0.02\n",
      "Episode 239, Reward: -5.656980518527797, Epsilon: 0.02\n",
      "Episode 240, Reward: -10.99252493903379, Epsilon: 0.02\n",
      "Episode 241, Reward: -7.759399815576531, Epsilon: 0.02\n",
      "Episode 242, Reward: -15.747125808724338, Epsilon: 0.02\n",
      "Episode 243, Reward: -12.365496989764361, Epsilon: 0.02\n",
      "Episode 244, Reward: 0.0, Epsilon: 0.02\n",
      "Episode 245, Reward: -11.044118558933013, Epsilon: 0.02\n",
      "Episode 246, Reward: -13.955724070269053, Epsilon: 0.02\n",
      "Episode 247, Reward: -7.287755773989061, Epsilon: 0.02\n",
      "Episode 248, Reward: -0.7317892264142802, Epsilon: 0.02\n",
      "Episode 249, Reward: -1.069811680707948, Epsilon: 0.02\n",
      "Episode 250, Reward: -14.332284084777891, Epsilon: 0.02\n",
      "Episode 251, Reward: 0.0, Epsilon: 0.02\n",
      "Episode 252, Reward: -5.0809818111644445, Epsilon: 0.02\n",
      "Episode 253, Reward: 148.81994252443982, Epsilon: 0.02\n",
      "Episode 254, Reward: -11.297221856024523, Epsilon: 0.02\n",
      "Episode 255, Reward: 0.0, Epsilon: 0.02\n",
      "Episode 256, Reward: -15.513100125678895, Epsilon: 0.02\n",
      "Episode 257, Reward: -3.4375055508087438, Epsilon: 0.02\n",
      "Episode 258, Reward: -3.067490837410404, Epsilon: 0.02\n",
      "Episode 259, Reward: -2.5550032841272476, Epsilon: 0.02\n",
      "Episode 260, Reward: -4.844944798251904, Epsilon: 0.02\n",
      "Episode 261, Reward: 0.5873292774867496, Epsilon: 0.02\n",
      "Episode 262, Reward: -5.097961001838293, Epsilon: 0.02\n",
      "Episode 263, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 264, Reward: -5.861342674897547, Epsilon: 0.01\n",
      "Episode 265, Reward: -20.65027082867764, Epsilon: 0.01\n",
      "Episode 266, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 267, Reward: -13.676893940774743, Epsilon: 0.01\n",
      "Episode 268, Reward: -1.8721115450412267, Epsilon: 0.01\n",
      "Episode 269, Reward: -6.256062070015623, Epsilon: 0.01\n",
      "Episode 270, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 271, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 272, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 273, Reward: -13.349807619387036, Epsilon: 0.01\n",
      "Episode 274, Reward: 35.02070572813523, Epsilon: 0.01\n",
      "Episode 275, Reward: 26.312180396469603, Epsilon: 0.01\n",
      "Episode 276, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 277, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 278, Reward: -0.48241415170099233, Epsilon: 0.01\n",
      "Episode 279, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 280, Reward: 57.08592362317312, Epsilon: 0.01\n",
      "Episode 281, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 282, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 283, Reward: 18.471873413063964, Epsilon: 0.01\n",
      "Episode 284, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 285, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 286, Reward: -8.880155803376908, Epsilon: 0.01\n",
      "Episode 287, Reward: -13.081480395690638, Epsilon: 0.01\n",
      "Episode 288, Reward: -12.317123997358756, Epsilon: 0.01\n",
      "Episode 289, Reward: -11.327149458094464, Epsilon: 0.01\n",
      "Episode 290, Reward: -13.03093607717082, Epsilon: 0.01\n",
      "Episode 291, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 292, Reward: -20.419185981946303, Epsilon: 0.01\n",
      "Episode 293, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 294, Reward: 2.275804537519967, Epsilon: 0.01\n",
      "Episode 295, Reward: -12.348453479479968, Epsilon: 0.01\n",
      "Episode 296, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 297, Reward: -10.042665511640413, Epsilon: 0.01\n",
      "Episode 298, Reward: 7.81381282840861, Epsilon: 0.01\n",
      "Episode 299, Reward: 11.934180849971703, Epsilon: 0.01\n",
      "Episode 300, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 301, Reward: 0.32443390641708, Epsilon: 0.01\n",
      "Episode 302, Reward: -7.419411749327858, Epsilon: 0.01\n",
      "Episode 303, Reward: -11.160782601039703, Epsilon: 0.01\n",
      "Episode 304, Reward: -6.30234901042582, Epsilon: 0.01\n",
      "Episode 305, Reward: -17.249310556722094, Epsilon: 0.01\n",
      "Episode 306, Reward: -15.816174533464194, Epsilon: 0.01\n",
      "Episode 307, Reward: -2.0268680282712714, Epsilon: 0.01\n",
      "Episode 308, Reward: 4.172633342431191, Epsilon: 0.01\n",
      "Episode 309, Reward: 2.3219821095381574, Epsilon: 0.01\n",
      "Episode 310, Reward: -11.281838686714849, Epsilon: 0.01\n",
      "Episode 311, Reward: -24.533137332605754, Epsilon: 0.01\n",
      "Episode 312, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 313, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 314, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 315, Reward: -7.441951602820672, Epsilon: 0.01\n",
      "Episode 316, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 317, Reward: -5.141111983172042, Epsilon: 0.01\n",
      "Episode 318, Reward: -3.71212847387538, Epsilon: 0.01\n",
      "Episode 319, Reward: -9.683600231980407, Epsilon: 0.01\n",
      "Episode 320, Reward: -8.522158791627312, Epsilon: 0.01\n",
      "Episode 321, Reward: -22.942375985028974, Epsilon: 0.01\n",
      "Episode 322, Reward: -3.8182739860609285, Epsilon: 0.01\n",
      "Episode 323, Reward: 45.012075980468715, Epsilon: 0.01\n",
      "Episode 324, Reward: 77.88477472824549, Epsilon: 0.01\n",
      "Episode 325, Reward: -1.6227449411552155, Epsilon: 0.01\n",
      "Episode 326, Reward: -2.3897647452131783, Epsilon: 0.01\n",
      "Episode 327, Reward: -8.084643055869751, Epsilon: 0.01\n",
      "Episode 328, Reward: -0.980953071535509, Epsilon: 0.01\n",
      "Episode 329, Reward: -4.872746213842371, Epsilon: 0.01\n",
      "Episode 330, Reward: 30.22172069407673, Epsilon: 0.01\n",
      "Episode 331, Reward: -13.057066193189202, Epsilon: 0.01\n",
      "Episode 332, Reward: 29.68721164739443, Epsilon: 0.01\n",
      "Episode 333, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 334, Reward: -7.863420536282555, Epsilon: 0.01\n",
      "Episode 335, Reward: -6.0700516376791605, Epsilon: 0.01\n",
      "Episode 336, Reward: -15.7558544616994, Epsilon: 0.01\n",
      "Episode 337, Reward: -17.08030522070164, Epsilon: 0.01\n",
      "Episode 338, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 339, Reward: -10.934743271627257, Epsilon: 0.01\n",
      "Episode 340, Reward: -12.40876932156004, Epsilon: 0.01\n",
      "Episode 341, Reward: -11.717631084027744, Epsilon: 0.01\n",
      "Episode 342, Reward: -2.3427389535351835, Epsilon: 0.01\n",
      "Episode 343, Reward: -7.3466564373257555, Epsilon: 0.01\n",
      "Episode 344, Reward: -17.61969208993033, Epsilon: 0.01\n",
      "Episode 345, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 346, Reward: 13.777913229779955, Epsilon: 0.01\n",
      "Episode 347, Reward: -2.9277552323069527, Epsilon: 0.01\n",
      "Episode 348, Reward: 3.1388533380984516, Epsilon: 0.01\n",
      "Episode 349, Reward: -0.02086232846215441, Epsilon: 0.01\n",
      "Episode 350, Reward: 1.6914661366470607, Epsilon: 0.01\n",
      "Episode 351, Reward: -5.9154900413251115, Epsilon: 0.01\n",
      "Episode 352, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 353, Reward: 15.506480037750798, Epsilon: 0.01\n",
      "Episode 354, Reward: 26.19184187739981, Epsilon: 0.01\n",
      "Episode 355, Reward: -6.9626873410644246, Epsilon: 0.01\n",
      "Episode 356, Reward: 50.02185421673558, Epsilon: 0.01\n",
      "Episode 357, Reward: -6.3367049229851755, Epsilon: 0.01\n",
      "Episode 358, Reward: -15.995487127081034, Epsilon: 0.01\n",
      "Episode 359, Reward: -0.7465498618424302, Epsilon: 0.01\n",
      "Episode 360, Reward: -6.947306980308267, Epsilon: 0.01\n",
      "Episode 361, Reward: -19.80383269963252, Epsilon: 0.01\n",
      "Episode 362, Reward: 6.198345464165226, Epsilon: 0.01\n",
      "Episode 363, Reward: -10.551789952978437, Epsilon: 0.01\n",
      "Episode 364, Reward: -7.223877098887435, Epsilon: 0.01\n",
      "Episode 365, Reward: 34.6472910725232, Epsilon: 0.01\n",
      "Episode 366, Reward: -13.899755082470648, Epsilon: 0.01\n",
      "Episode 367, Reward: 1.1797947833829703, Epsilon: 0.01\n",
      "Episode 368, Reward: -12.462220934149132, Epsilon: 0.01\n",
      "Episode 369, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 370, Reward: -3.7110986035444986, Epsilon: 0.01\n",
      "Episode 371, Reward: 36.86569334293927, Epsilon: 0.01\n",
      "Episode 372, Reward: -5.249388969867462, Epsilon: 0.01\n",
      "Episode 373, Reward: -4.828038618962996, Epsilon: 0.01\n",
      "Episode 374, Reward: 11.231148936182192, Epsilon: 0.01\n",
      "Episode 375, Reward: -8.382889302182795, Epsilon: 0.01\n",
      "Episode 376, Reward: -13.970044605232335, Epsilon: 0.01\n",
      "Episode 377, Reward: 8.330146215489268, Epsilon: 0.01\n",
      "Episode 378, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 379, Reward: -4.059518571174368, Epsilon: 0.01\n",
      "Episode 380, Reward: -7.531567065937863, Epsilon: 0.01\n",
      "Episode 381, Reward: -2.6011729655025664, Epsilon: 0.01\n",
      "Episode 382, Reward: 17.270035391489298, Epsilon: 0.01\n",
      "Episode 383, Reward: -1.2954796570851905, Epsilon: 0.01\n",
      "Episode 384, Reward: 5.586723934819379, Epsilon: 0.01\n",
      "Episode 385, Reward: 21.94414528630441, Epsilon: 0.01\n",
      "Episode 386, Reward: -11.756796663364021, Epsilon: 0.01\n",
      "Episode 387, Reward: 11.026149576577062, Epsilon: 0.01\n",
      "Episode 388, Reward: -17.94406567123707, Epsilon: 0.01\n",
      "Episode 389, Reward: 10.07647721932459, Epsilon: 0.01\n",
      "Episode 390, Reward: -8.157497686712274, Epsilon: 0.01\n",
      "Episode 391, Reward: 11.09225201084556, Epsilon: 0.01\n",
      "Episode 392, Reward: 5.511533637346045, Epsilon: 0.01\n",
      "Episode 393, Reward: -7.3234955211558, Epsilon: 0.01\n",
      "Episode 394, Reward: -16.801438226133275, Epsilon: 0.01\n",
      "Episode 395, Reward: 0.9543305640333566, Epsilon: 0.01\n",
      "Episode 396, Reward: -18.657127773027366, Epsilon: 0.01\n",
      "Episode 397, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 398, Reward: -4.1646591948054095, Epsilon: 0.01\n",
      "Episode 399, Reward: -10.360907191181754, Epsilon: 0.01\n",
      "Episode 400, Reward: -5.456215497353988, Epsilon: 0.01\n",
      "Episode 401, Reward: 0.8078947476296761, Epsilon: 0.01\n",
      "Episode 402, Reward: -17.731876750957294, Epsilon: 0.01\n",
      "Episode 403, Reward: -12.108783843505492, Epsilon: 0.01\n",
      "Episode 404, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 405, Reward: 37.196404981346895, Epsilon: 0.01\n",
      "Episode 406, Reward: -13.385516518703687, Epsilon: 0.01\n",
      "Episode 407, Reward: -3.105474388456914, Epsilon: 0.01\n",
      "Episode 408, Reward: -14.391778148423592, Epsilon: 0.01\n",
      "Episode 409, Reward: -4.645868705934443, Epsilon: 0.01\n",
      "Episode 410, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 411, Reward: 1.3424178182241457, Epsilon: 0.01\n",
      "Episode 412, Reward: 28.736536738157906, Epsilon: 0.01\n",
      "Episode 413, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 414, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 415, Reward: 13.848157061073294, Epsilon: 0.01\n",
      "Episode 416, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 417, Reward: 10.70861007099209, Epsilon: 0.01\n",
      "Episode 418, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 419, Reward: -18.925590452916452, Epsilon: 0.01\n",
      "Episode 420, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 421, Reward: 0.8706390234374611, Epsilon: 0.01\n",
      "Episode 422, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 423, Reward: -8.282174056299562, Epsilon: 0.01\n",
      "Episode 424, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 425, Reward: -5.91943406027818, Epsilon: 0.01\n",
      "Episode 426, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 427, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 428, Reward: -4.454148083872855, Epsilon: 0.01\n",
      "Episode 429, Reward: -11.768113584717, Epsilon: 0.01\n",
      "Episode 430, Reward: -14.009201952750916, Epsilon: 0.01\n",
      "Episode 431, Reward: 45.30603148260371, Epsilon: 0.01\n",
      "Episode 432, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 433, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 434, Reward: -5.08321147750636, Epsilon: 0.01\n",
      "Episode 435, Reward: -1.3900744512939993, Epsilon: 0.01\n",
      "Episode 436, Reward: -10.244331551849978, Epsilon: 0.01\n",
      "Episode 437, Reward: -1.1295430841361145, Epsilon: 0.01\n",
      "Episode 438, Reward: -20.81960358184892, Epsilon: 0.01\n",
      "Episode 439, Reward: -10.256050357951601, Epsilon: 0.01\n",
      "Episode 440, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 441, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 442, Reward: -14.800206141108063, Epsilon: 0.01\n",
      "Episode 443, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 444, Reward: 10.52503985002057, Epsilon: 0.01\n",
      "Episode 445, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 446, Reward: -9.178132975427651, Epsilon: 0.01\n",
      "Episode 447, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 448, Reward: 2.30124680773636, Epsilon: 0.01\n",
      "Episode 449, Reward: -18.162275745381578, Epsilon: 0.01\n",
      "Episode 450, Reward: -13.9474208254818, Epsilon: 0.01\n",
      "Episode 451, Reward: -19.245427160237607, Epsilon: 0.01\n",
      "Episode 452, Reward: -38.535023409223506, Epsilon: 0.01\n",
      "Episode 453, Reward: -6.674961297811799, Epsilon: 0.01\n",
      "Episode 454, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 455, Reward: 46.677895822184176, Epsilon: 0.01\n",
      "Episode 456, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 457, Reward: 17.313645867034303, Epsilon: 0.01\n",
      "Episode 458, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 459, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 460, Reward: -21.694229462641754, Epsilon: 0.01\n",
      "Episode 461, Reward: -19.28419311169834, Epsilon: 0.01\n",
      "Episode 462, Reward: -13.899169234027177, Epsilon: 0.01\n",
      "Episode 463, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 464, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 465, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 466, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 467, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 468, Reward: -16.84601224561211, Epsilon: 0.01\n",
      "Episode 469, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 470, Reward: -11.59982616500466, Epsilon: 0.01\n",
      "Episode 471, Reward: -3.5264020533915112, Epsilon: 0.01\n",
      "Episode 472, Reward: -10.924640834905633, Epsilon: 0.01\n",
      "Episode 473, Reward: -16.854905914527038, Epsilon: 0.01\n",
      "Episode 474, Reward: 36.12156835198741, Epsilon: 0.01\n",
      "Episode 475, Reward: -13.210924317559234, Epsilon: 0.01\n",
      "Episode 476, Reward: 27.7763503662616, Epsilon: 0.01\n",
      "Episode 477, Reward: -6.961280885012155, Epsilon: 0.01\n",
      "Episode 478, Reward: 6.309814904446467, Epsilon: 0.01\n",
      "Episode 479, Reward: 34.11373584961855, Epsilon: 0.01\n",
      "Episode 480, Reward: -16.044034412735567, Epsilon: 0.01\n",
      "Episode 481, Reward: 10.213200193080445, Epsilon: 0.01\n",
      "Episode 482, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 483, Reward: -5.883997897215563, Epsilon: 0.01\n",
      "Episode 484, Reward: -16.99065903028119, Epsilon: 0.01\n",
      "Episode 485, Reward: -9.311711301158997, Epsilon: 0.01\n",
      "Episode 486, Reward: -3.4727325296045883, Epsilon: 0.01\n",
      "Episode 487, Reward: -10.279633061414666, Epsilon: 0.01\n",
      "Episode 488, Reward: -9.989595156151726, Epsilon: 0.01\n",
      "Episode 489, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 490, Reward: -11.42042994556042, Epsilon: 0.01\n",
      "Episode 491, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 492, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 493, Reward: 14.877812058872095, Epsilon: 0.01\n",
      "Episode 494, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 495, Reward: -5.693521074190094, Epsilon: 0.01\n",
      "Episode 496, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 497, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 498, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 499, Reward: 28.150097795182948, Epsilon: 0.01\n",
      "Episode 500, Reward: -2.109376880519642, Epsilon: 0.01\n",
      "Episode 501, Reward: -11.417153148803244, Epsilon: 0.01\n",
      "Episode 502, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 503, Reward: -2.0184051523929334, Epsilon: 0.01\n",
      "Episode 504, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 505, Reward: 30.25509276405495, Epsilon: 0.01\n",
      "Episode 506, Reward: -5.160111138714583, Epsilon: 0.01\n",
      "Episode 507, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 508, Reward: 75.83865069163201, Epsilon: 0.01\n",
      "Episode 509, Reward: 10.697553779423307, Epsilon: 0.01\n",
      "Episode 510, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 511, Reward: -18.96500452519219, Epsilon: 0.01\n",
      "Episode 512, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 513, Reward: -12.915446872519222, Epsilon: 0.01\n",
      "Episode 514, Reward: 30.69960529904288, Epsilon: 0.01\n",
      "Episode 515, Reward: -4.741217007787135, Epsilon: 0.01\n",
      "Episode 516, Reward: 31.93340997732385, Epsilon: 0.01\n",
      "Episode 517, Reward: -10.99442562751148, Epsilon: 0.01\n",
      "Episode 518, Reward: -5.741684487309481, Epsilon: 0.01\n",
      "Episode 519, Reward: 7.1693888406184625, Epsilon: 0.01\n",
      "Episode 520, Reward: -12.13313224287606, Epsilon: 0.01\n",
      "Episode 521, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 522, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 523, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 524, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 525, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 526, Reward: -11.806183281169975, Epsilon: 0.01\n",
      "Episode 527, Reward: 4.835764712984213, Epsilon: 0.01\n",
      "Episode 528, Reward: -6.720236168094628, Epsilon: 0.01\n",
      "Episode 529, Reward: -18.840439122451475, Epsilon: 0.01\n",
      "Episode 530, Reward: -7.429725125237376, Epsilon: 0.01\n",
      "Episode 531, Reward: 12.699357469281276, Epsilon: 0.01\n",
      "Episode 532, Reward: -5.261548318810887, Epsilon: 0.01\n",
      "Episode 533, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 534, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 535, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 536, Reward: 44.58144602534922, Epsilon: 0.01\n",
      "Episode 537, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 538, Reward: -13.482067290643315, Epsilon: 0.01\n",
      "Episode 539, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 540, Reward: -3.9221738739400154, Epsilon: 0.01\n",
      "Episode 541, Reward: -9.288056145913684, Epsilon: 0.01\n",
      "Episode 542, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 543, Reward: 0.745537479334986, Epsilon: 0.01\n",
      "Episode 544, Reward: 56.87862732314518, Epsilon: 0.01\n",
      "Episode 545, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 546, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 547, Reward: -1.3893022877794587, Epsilon: 0.01\n",
      "Episode 548, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 549, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 550, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 551, Reward: 6.184536969933485, Epsilon: 0.01\n",
      "Episode 552, Reward: -10.530066601364744, Epsilon: 0.01\n",
      "Episode 553, Reward: -1.3712924957004278, Epsilon: 0.01\n",
      "Episode 554, Reward: -8.337033987434992, Epsilon: 0.01\n",
      "Episode 555, Reward: 26.409230667037374, Epsilon: 0.01\n",
      "Episode 556, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 557, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 558, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 559, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 560, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 561, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 562, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 563, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 564, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 565, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 566, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 567, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 568, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 569, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 570, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 571, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 572, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 573, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 574, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 575, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 576, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 577, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 578, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 579, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 580, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 581, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 582, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 583, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 584, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 585, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 586, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 587, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 588, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 589, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 590, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 591, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 592, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 593, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 594, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 595, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 596, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 597, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 598, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 599, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 600, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 601, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 602, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 603, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 604, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 605, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 606, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 607, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 608, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 609, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 610, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 611, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 612, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 613, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 614, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 615, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 616, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 617, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 618, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 619, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 620, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 621, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 622, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 623, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 624, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 625, Reward: -15.202680767105171, Epsilon: 0.01\n",
      "Episode 626, Reward: -15.184704835082249, Epsilon: 0.01\n",
      "Episode 627, Reward: 22.608197811763954, Epsilon: 0.01\n",
      "Episode 628, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 629, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 630, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 631, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 632, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 633, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 634, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 635, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 636, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 637, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 638, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 639, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 640, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 641, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 642, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 643, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 644, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 645, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 646, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 647, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 648, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 649, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 650, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 651, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 652, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 653, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 654, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 655, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 656, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 657, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 658, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 659, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 660, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 661, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 662, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 663, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 664, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 665, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 666, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 667, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 668, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 669, Reward: -4.494922642458182, Epsilon: 0.01\n",
      "Episode 670, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 671, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 672, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 673, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 674, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 675, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 676, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 677, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 678, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 679, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 680, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 681, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 682, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 683, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 684, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 685, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 686, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 687, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 688, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 689, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 690, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 691, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 692, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 693, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 694, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 695, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 696, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 697, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 698, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 699, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 700, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 701, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 702, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 703, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 704, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 705, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 706, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 707, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 708, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 709, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 710, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 711, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 712, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 713, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 714, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 715, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 716, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 717, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 718, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 719, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 720, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 721, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 722, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 723, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 724, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 725, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 726, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 727, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 728, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 729, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 730, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 731, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 732, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 733, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 734, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 735, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 736, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 737, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 738, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 739, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 740, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 741, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 742, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 743, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 744, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 745, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 746, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 747, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 748, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 749, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 750, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 751, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 752, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 753, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 754, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 755, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 756, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 757, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 758, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 759, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 760, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 761, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 762, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 763, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 764, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 765, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 766, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 767, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 768, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 769, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 770, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 771, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 772, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 773, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 774, Reward: 3.272521340467963, Epsilon: 0.01\n",
      "Episode 775, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 776, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 777, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 778, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 779, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 780, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 781, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 782, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 783, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 784, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 785, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 786, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 787, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 788, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 789, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 790, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 791, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 792, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 793, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 794, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 795, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 796, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 797, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 798, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 799, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 800, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 801, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 802, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 803, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 804, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 805, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 806, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 807, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 808, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 809, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 810, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 811, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 812, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 813, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 814, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 815, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 816, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 817, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 818, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 819, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 820, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 821, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 822, Reward: 43.017000461071596, Epsilon: 0.01\n",
      "Episode 823, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 824, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 825, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 826, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 827, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 828, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 829, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 830, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 831, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 832, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 833, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 834, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 835, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 836, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 837, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 838, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 839, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 840, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 841, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 842, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 843, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 844, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 845, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 846, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 847, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 848, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 849, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 850, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 851, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 852, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 853, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 854, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 855, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 856, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 857, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 858, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 859, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 860, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 861, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 862, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 863, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 864, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 865, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 866, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 867, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 868, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 869, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 870, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 871, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 872, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 873, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 874, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 875, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 876, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 877, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 878, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 879, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 880, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 881, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 882, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 883, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 884, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 885, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 886, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 887, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 888, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 889, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 890, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 891, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 892, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 893, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 894, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 895, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 896, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 897, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 898, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 899, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 900, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 901, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 902, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 903, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 904, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 905, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 906, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 907, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 908, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 909, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 910, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 911, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 912, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 913, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 914, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 915, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 916, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 917, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 918, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 919, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 920, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 921, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 922, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 923, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 924, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 925, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 926, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 927, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 928, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 929, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 930, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 931, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 932, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 933, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 934, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 935, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 936, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 937, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 938, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 939, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 940, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 941, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 942, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 943, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 944, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 945, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 946, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 947, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 948, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 949, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 950, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 951, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 952, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 953, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 954, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 955, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 956, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 957, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 958, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 959, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 960, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 961, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 962, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 963, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 964, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 965, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 966, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 967, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 968, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 969, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 970, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 971, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 972, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 973, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 974, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 975, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 976, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 977, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 978, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 979, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 980, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 981, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 982, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 983, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 984, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 985, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 986, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 987, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 988, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 989, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 990, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 991, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 992, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 993, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 994, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 995, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 996, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 997, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 998, Reward: 0.0, Epsilon: 0.01\n",
      "Episode 999, Reward: 0.0, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "env = SimEnv({\"df\": df})  # ✅ Use your trading environment\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "num_episodes = 1000\n",
    "update_target_every = 50  # Update target network every 50 episodes\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        valid_actions = env.valid_actions()  # ✅ Get valid actions\n",
    "        action = agent.select_action(state, valid_actions)  # ✅ Use masked action selection\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        #print(f\"next_state={next_state}, reward={reward}, done={done}, truncated={truncated}, info={info}\")\n",
    "\n",
    "        agent.store_transition((state, action, reward, next_state, done))\n",
    "        agent.train()\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    if episode % update_target_every == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}, Epsilon: {agent.epsilon:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
